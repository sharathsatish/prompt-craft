# Quality Pyramid Assessment Tool

*Systematic evaluation framework for prompt quality using Quality Pyramid methodology*

## Tool Overview

This assessment tool provides a structured approach to evaluating prompt quality using the Quality Pyramid
principles (Completeness ‚Üí Accuracy ‚Üí Relevance ‚Üí Efficiency). Use this tool to validate new prompts,
improve existing ones, and maintain quality standards across your prompt library.

## Quick Assessment Checklist

### ‚úÖ Completeness Foundation (25 points)

**Context and Information (10 points)**

- [ ] All necessary background information included (3 pts)
- [ ] Clear purpose and objectives stated (3 pts)
- [ ] Success criteria explicitly defined (2 pts)
- [ ] Constraints and limitations specified (2 pts)

**CREATE Framework Application (15 points)**

- [ ] Character: Role and expertise clearly defined (3 pts)
- [ ] Request: Specific, actionable objectives (3 pts)
- [ ] Examples: High-quality demonstrations provided (3 pts)
- [ ] Adjustments: Clear constraints and standards (3 pts)
- [ ] Type: Output format specified (1.5 pts)
- [ ] Extras: Additional context included (1.5 pts)

### üéØ Accuracy Validation (25 points)

**Technical Correctness (10 points)**

- [ ] Instructions are technically accurate (5 pts)
- [ ] Examples demonstrate correct patterns (3 pts)
- [ ] No conflicting or contradictory guidance (2 pts)

**Best Practices Alignment (15 points)**

- [ ] Follows established prompt engineering principles (5 pts)
- [ ] Incorporates proven techniques and patterns (5 pts)
- [ ] Avoids common pitfalls and anti-patterns (5 pts)

### üîç Relevance Optimization (25 points)

**Audience Alignment (10 points)**

- [ ] Language appropriate for target users (3 pts)
- [ ] Complexity matches audience skill level (3 pts)
- [ ] Context relevant to audience needs (2 pts)
- [ ] Cultural and organizational considerations (2 pts)

**Use Case Fit (15 points)**

- [ ] Directly addresses intended use case (7 pts)
- [ ] Scope appropriate for the situation (4 pts)
- [ ] Outputs match stakeholder expectations (4 pts)

### ‚ö° Efficiency Engineering (25 points)

**Performance Optimization (10 points)**

- [ ] Prompt length optimized for purpose (3 pts)
- [ ] Clear, concise language without redundancy (3 pts)
- [ ] Structured for easy AI processing (2 pts)
- [ ] Includes only essential information (2 pts)

**Maintainability (15 points)**

- [ ] Easy to understand and modify (5 pts)
- [ ] Reusable across similar contexts (5 pts)
- [ ] Well-documented and self-explanatory (3 pts)
- [ ] Version control and iteration friendly (2 pts)

## Scoring Guide

### Total Score Interpretation

**90-100 points: Excellent Quality**

- Ready for production use
- Suitable for organizational standards
- Can serve as template for similar prompts

**75-89 points: Good Quality**

- Minor improvements needed
- Suitable for most use cases
- Review specific areas scoring below 20/25

**60-74 points: Adequate Quality**  

- Significant improvements required
- Focus on lowest-scoring pyramid levels
- Consider redesign using CREATE Framework

**Below 60 points: Needs Major Revision**

- Return to CREATE Framework basics
- Redesign from foundation up
- Seek guidance or use existing templates

### Pyramid Level Analysis

**Completeness Issues (Below 20/25)**

- Incomplete context or missing information
- CREATE Framework elements not fully developed
- Unclear objectives or success criteria

**Accuracy Issues (Below 20/25)**

- Technical errors or outdated information
- Poor examples or contradictory guidance
- Doesn't follow best practices

**Relevance Issues (Below 20/25)**

- Audience mismatch or inappropriate complexity
- Doesn't address actual use case needs
- Missing cultural or contextual considerations

**Efficiency Issues (Below 20/25)**

- Too verbose or contains unnecessary information
- Difficult to understand or maintain
- Not optimized for AI processing

## Detailed Assessment Framework

### Completeness Deep Dive

#### Context Evaluation Questions

1. **Background Information**
   - Is the purpose clearly stated?
   - Are prerequisites and assumptions documented?
   - Is the scope and boundaries defined?

2. **Success Criteria**
   - How will success be measured?
   - What constitutes acceptable output?
   - Are quality standards specified?

3. **Constraint Documentation**
   - What limitations exist?
   - Are there compliance requirements?
   - What should be avoided?

#### CREATE Framework Quality Check

**Character Assessment:**

```markdown
Quality Indicators:
- Role clearly defined with specific expertise ‚úì/‚ùå
- Appropriate authority level for the task ‚úì/‚ùå 
- Expertise matches task requirements ‚úì/‚ùå
- Professional context appropriate ‚úì/‚ùå

Improvement Opportunities:
- [Specific suggestions for character enhancement]
```

**Request Assessment:**

```markdown
Quality Indicators:
- Action verbs specific and clear ‚úì/‚ùå
- Objectives measurable and concrete ‚úì/‚ùå
- Scope appropriately bounded ‚úì/‚ùå
- Success criteria explicit ‚úì/‚ùå

Improvement Opportunities:
- [Specific suggestions for request enhancement]
```

**Examples Assessment:**

```markdown
Quality Indicators:
- Examples demonstrate desired pattern ‚úì/‚ùå
- Sufficient variety to show adaptability ‚úì/‚ùå
- Clear input/output relationships ‚úì/‚ùå
- Educational value beyond copying ‚úì/‚ùå

Improvement Opportunities:
- [Specific suggestions for examples enhancement]
```

**Adjustments Assessment:**

```markdown
Quality Indicators:
- Constraints clearly stated ‚úì/‚ùå
- Quality standards specified ‚úì/‚ùå
- Boundaries and limitations defined ‚úì/‚ùå
- Guardrails prevent common errors ‚úì/‚ùå

Improvement Opportunities:
- [Specific suggestions for adjustments enhancement]
```

**Type Assessment:**

```markdown
Quality Indicators:
- Output format clearly specified ‚úì/‚ùå
- Structure meets use case needs ‚úì/‚ùå
- Validation criteria included ‚úì/‚ùå
- Delivery method appropriate ‚úì/‚ùå

Improvement Opportunities:
- [Specific suggestions for type enhancement]
```

**Extras Assessment:**

```markdown
Quality Indicators:
- Additional context provided ‚úì/‚ùå
- Special requirements addressed ‚úì/‚ùå
- Edge cases considered ‚úì/‚ùå
- Background information sufficient ‚úì/‚ùå

Improvement Opportunities:
- [Specific suggestions for extras enhancement]
```

### Accuracy Deep Dive

#### Technical Validation Checklist

- [ ] All technical terms used correctly
- [ ] Procedures follow current best practices
- [ ] Examples show correct implementation
- [ ] No deprecated or outdated approaches
- [ ] Cross-platform compatibility considered

#### Best Practices Verification

- [ ] Follows established prompt engineering principles
- [ ] Incorporates proven design patterns
- [ ] Avoids known anti-patterns and pitfalls
- [ ] Includes appropriate safety measures
- [ ] Considers ethical implications

### Relevance Deep Dive

#### Audience Analysis Framework

**Skill Level Alignment:**

```markdown
Target Audience: [Define primary users]
Skill Requirements: [What they need to know]
Language Level: [Technical vs. accessible]
Context Familiarity: [Assumed background knowledge]

Alignment Check:
- Complexity appropriate for skill level ‚úì/‚ùå
- Terminology matches audience vocabulary ‚úì/‚ùå
- Examples relatable to their experience ‚úì/‚ùå
- Instructions match their workflow ‚úì/‚ùå
```

**Use Case Validation:**

```markdown
Primary Use Case: [Main intended application]
Secondary Uses: [Additional applications]
Context Requirements: [Situational factors]

Validation Questions:
- Does the prompt directly address the primary use case?
- Are outputs useful for intended stakeholders?
- Does the scope match the problem complexity?
- Are all critical success factors addressed?
```

### Efficiency Deep Dive

#### Performance Optimization Analysis

**Prompt Length Assessment:**

- Total word count: [Count]
- Essential vs. optional content ratio: [Ratio]
- Information density: [High/Medium/Low]
- Processing complexity: [Simple/Moderate/Complex]

**Clarity and Conciseness:**

```markdown
Readability Factors:
- Average sentence length: [Length]
- Technical jargon level: [Assessment]
- Structural organization: [Rating]
- Cognitive load: [Low/Medium/High]

Optimization Opportunities:
- [Specific suggestions for improvement]
```

#### Maintainability Assessment

**Modification Ease:**

- [ ] Structure supports easy updates
- [ ] Modular design allows component changes
- [ ] Documentation supports understanding
- [ ] Change history can be tracked

**Reusability Potential:**

- [ ] Core elements adaptable to similar contexts
- [ ] Template potential for related use cases
- [ ] Scaling considerations documented
- [ ] Customization guidance provided

## Quality Improvement Workflow

### Step 1: Initial Assessment

1. Complete the Quick Assessment Checklist
2. Calculate total score and pyramid level scores
3. Identify the lowest-scoring areas
4. Determine overall quality level

### Step 2: Targeted Analysis

1. Focus on pyramid levels scoring below 20/25
2. Use detailed assessment frameworks for problem areas
3. Identify specific improvement opportunities
4. Prioritize changes by impact and effort

### Step 3: Improvement Implementation

1. Address Completeness issues first (foundation)
2. Fix Accuracy problems (correctness)
3. Optimize for Relevance (fit)
4. Enhance Efficiency (performance)

### Step 4: Validation and Iteration

1. Re-assess using the quality checklist
2. Test with target audience or use cases
3. Document improvements and lessons learned
4. Plan for ongoing maintenance and updates

## Assessment Templates

### Individual Prompt Assessment

```markdown
# Prompt Quality Assessment

**Prompt Title:** [Name/Description]
**Assessor:** [Your Name]
**Date:** [Assessment Date]
**Version:** [Prompt Version]

## Quick Assessment Results

| Quality Pyramid Level | Score | Max | Percentage |
|----------------------|-------|-----|------------|
| Completeness         | __/25 | 25  | __%        |
| Accuracy            | __/25 | 25  | __%        |
| Relevance           | __/25 | 25  | __%        |
| Efficiency          | __/25 | 25  | __%        |
| **Total**           | __/100| 100 | __%        |

## Quality Level: [Excellent/Good/Adequate/Needs Revision]

## Key Findings

### Strengths
- [What works well]
- [Strong areas]
- [Best practices demonstrated]

### Improvement Opportunities
1. **Priority 1 (Critical):** [Most important changes]
2. **Priority 2 (Important):** [Secondary improvements]
3. **Priority 3 (Enhancement):** [Nice-to-have improvements]

## Detailed Analysis

### Completeness (Score: __/25)
[Specific findings and recommendations]

### Accuracy (Score: __/25)
[Specific findings and recommendations]

### Relevance (Score: __/25)
[Specific findings and recommendations]

### Efficiency (Score: __/25)
[Specific findings and recommendations]

## Action Plan

### Immediate Actions (Next 24 hours)
- [ ] [Action item 1]
- [ ] [Action item 2]

### Short-term Actions (Next week)
- [ ] [Action item 1]
- [ ] [Action item 2]

### Long-term Actions (Next month)
- [ ] [Action item 1]
- [ ] [Action item 2]

## Follow-up Assessment

**Planned Date:** [When to reassess]
**Success Criteria:** [How to measure improvement]
**Review Focus:** [Areas to pay special attention to]
```

### Team Prompt Library Assessment

```markdown
# Prompt Library Quality Assessment

**Library/Collection:** [Name]
**Assessment Period:** [Date Range]
**Team/Organization:** [Context]
**Assessor(s):** [Names]

## Library Overview

**Total Prompts Assessed:** [Number]
**Assessment Method:** [Sampling strategy if applicable]
**Quality Standards:** [Organizational requirements]

## Aggregate Results

### Quality Distribution
- Excellent (90-100): [Number] ([Percentage]%)
- Good (75-89): [Number] ([Percentage]%)
- Adequate (60-74): [Number] ([Percentage]%)
- Needs Revision (<60): [Number] ([Percentage]%)

### Pyramid Level Analysis
| Level | Average Score | Std Dev | Range |
|-------|---------------|---------|-------|
| Completeness | [Score]/25 | [Dev] | [Range] |
| Accuracy | [Score]/25 | [Dev] | [Range] |
| Relevance | [Score]/25 | [Dev] | [Range] |
| Efficiency | [Score]/25 | [Dev] | [Range] |

## Common Issues Identified

### Completeness Issues
- [Pattern 1]: [Frequency] - [Impact]
- [Pattern 2]: [Frequency] - [Impact]

### Accuracy Issues
- [Pattern 1]: [Frequency] - [Impact]
- [Pattern 2]: [Frequency] - [Impact]

### Relevance Issues
- [Pattern 1]: [Frequency] - [Impact]
- [Pattern 2]: [Frequency] - [Impact]

### Efficiency Issues
- [Pattern 1]: [Frequency] - [Impact]
- [Pattern 2]: [Frequency] - [Impact]

## Recommendations

### Library-Wide Improvements
1. **Training Needs:** [Skills to develop]
2. **Template Updates:** [Standard patterns to improve]
3. **Process Changes:** [Workflow modifications]
4. **Quality Standards:** [Standard updates needed]

### High-Priority Prompts for Revision
1. [Prompt Name] - [Primary Issues] - [Business Impact]
2. [Prompt Name] - [Primary Issues] - [Business Impact]

### Success Templates to Promote
1. [Prompt Name] - [Why it's exemplary] - [Replication strategy]
2. [Prompt Name] - [Why it's exemplary] - [Replication strategy]

## Implementation Plan

### Phase 1: Critical Issues (Immediate)
- [ ] [Action] - [Owner] - [Deadline]
- [ ] [Action] - [Owner] - [Deadline]

### Phase 2: Systematic Improvements (30 days)
- [ ] [Action] - [Owner] - [Deadline]
- [ ] [Action] - [Owner] - [Deadline]

### Phase 3: Optimization (60 days)
- [ ] [Action] - [Owner] - [Deadline]
- [ ] [Action] - [Owner] - [Deadline]

## Success Metrics

### Quality Targets (Next Assessment)
- Excellent prompts: [Target]% (Current: [Current]%)
- Average completeness score: [Target]/25 (Current: [Current]/25)
- Average accuracy score: [Target]/25 (Current: [Current]/25)
- Average relevance score: [Target]/25 (Current: [Current]/25)
- Average efficiency score: [Target]/25 (Current: [Current]/25)

### Process Metrics
- Assessment frequency: [Target frequency]
- Team adoption of quality standards: [Measurement method]
- Training completion: [Target percentage]
- Template usage: [Measurement method]
```

---

*Quality assessment is most effective when conducted regularly and used to drive continuous improvement. Focus on
systematic enhancement rather than perfection.*
